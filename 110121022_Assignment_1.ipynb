{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOHVCk9iSZBrCYBYBe2AW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bipeline-12/CSOE11---Big-Data-Analytics/blob/main/110121022_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_custom_data(input_file):\n",
        "    # Open the file and read its contents\n",
        "    with open(input_file, 'r') as file_input:\n",
        "        custom_data = file_input.readlines()\n",
        "    # Split each line using ';' and create a list of transactions\n",
        "    custom_transactions = [line.strip().split(';') for line in custom_data]\n",
        "    return custom_transactions\n",
        "\n",
        "def load_custom_data(input_file):\n",
        "    # Open the file and read its contents\n",
        "    with open(input_file, 'r') as file_input:\n",
        "        custom_data = file_input.readlines()\n",
        "    # Split each line using ';' and create a list of transactions\n",
        "    custom_transactions = [line.strip().split(';') for line in custom_data]\n",
        "    return custom_transactions\n",
        "\n",
        "def generate_initial_candidates(transactions):\n",
        "    initial_candidates = []\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            if [item] not in initial_candidates:\n",
        "                initial_candidates.append([item])\n",
        "    initial_candidates.sort()\n",
        "    return list(map(frozenset, initial_candidates))\n",
        "\n",
        "def scan_for_frequent_items(transactions, candidates, min_support):\n",
        "    subset_counts = {}\n",
        "    for transaction in transactions:\n",
        "        for candidate in candidates:\n",
        "            if candidate.issubset(transaction):\n",
        "                if not candidate in subset_counts:\n",
        "                    subset_counts[candidate] = 1\n",
        "                else:\n",
        "                    subset_counts[candidate] += 1\n",
        "    num_transactions = float(len(transactions))\n",
        "    frequent_itemsets = []\n",
        "    support_data = {}\n",
        "    for key in subset_counts:\n",
        "        support = subset_counts[key] / num_transactions\n",
        "        if support >= min_support:\n",
        "            frequent_itemsets.insert(0, key)\n",
        "        support_data[key] = support\n",
        "    return frequent_itemsets, support_data\n",
        "\n",
        "def custom_apriori_algorithm(transactions, min_support=0.01):\n",
        "    initial_candidates = generate_initial_candidates(transactions)\n",
        "    frequent_itemsets_1, support_data = scan_for_frequent_items(transactions, initial_candidates, min_support)\n",
        "    frequent_itemsets = [frequent_itemsets_1]\n",
        "    k = 2\n",
        "    while (len(frequent_itemsets[k - 2]) > 0):\n",
        "        candidates_k = generate_candidates(frequent_itemsets[k - 2], k)\n",
        "        frequent_itemsets_k, support_data_k = scan_for_frequent_items(transactions, candidates_k, min_support)\n",
        "        support_data.update(support_data_k)\n",
        "        frequent_itemsets.append(frequent_itemsets_k)\n",
        "        k += 1\n",
        "    return frequent_itemsets, support_data\n",
        "\n",
        "def generate_candidates(frequent_itemsets, k):\n",
        "    candidates = []\n",
        "    len_frequent_itemsets = len(frequent_itemsets)\n",
        "    for i in range(len_frequent_itemsets):\n",
        "        for j in range(i + 1, len_frequent_itemsets):\n",
        "            L1 = list(frequent_itemsets[i])[:k - 2]\n",
        "            L2 = list(frequent_itemsets[j])[:k - 2]\n",
        "            L1.sort()\n",
        "            L2.sort()\n",
        "            if L1 == L2:\n",
        "                candidates.append(frequent_itemsets[i] | frequent_itemsets[j])\n",
        "    return candidates\n",
        "\n",
        "def sort_and_insert(file):\n",
        "    with open(file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    custom_patterns = []\n",
        "    for line in lines:\n",
        "        pattern, support = line.strip().split(':')\n",
        "        custom_patterns.append((pattern.strip(), support.strip()))\n",
        "    custom_patterns.sort(key=lambda x: float(x[1]), reverse=True)\n",
        "    with open(file, 'w') as f:\n",
        "        for pattern, support in custom_patterns:\n",
        "            f.write(f\"{pattern.strip()} : {support.strip()}\\n\")\n",
        "\n",
        "def insert_total_count_in_file(file):\n",
        "    with open(file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    total_count = len(lines)\n",
        "    with open(file, 'w') as f:\n",
        "        f.write(f\"#Total count: {total_count}\\n\")\n",
        "        for line in lines:\n",
        "            f.write(line)\n",
        "\n",
        "def run_custom_analysis(input_file):\n",
        "    # Load the custom dataset\n",
        "    custom_dataset = load_custom_data(input_file)\n",
        "\n",
        "    # Apply the custom Apriori algorithm\n",
        "    frequent_itemsets, support_data = custom_apriori_algorithm(custom_dataset)\n",
        "\n",
        "    # Write patterns_1.txt\n",
        "    with open('patterns_1.txt', 'w') as f:\n",
        "        for itemset in frequent_itemsets[0]:\n",
        "            f.write(f\"{list(itemset)[0]} : {support_data[itemset]}\\n\")\n",
        "    sort_and_insert('patterns_1.txt')\n",
        "    insert_total_count_in_file('patterns_1.txt')\n",
        "\n",
        "    # Write patterns_all.txt\n",
        "    with open('patterns_all.txt', 'w') as f:\n",
        "        for k_itemsets in frequent_itemsets:\n",
        "            for itemset in k_itemsets:\n",
        "                f.write(f\"{';'.join(list(itemset))} : {support_data[itemset]}\\n\")\n",
        "    sort_and_insert('patterns_all.txt')\n",
        "    insert_total_count_in_file('patterns_all.txt')\n",
        "\n",
        "    def generate_closed_itemsets(frequent_itemsets, support_data):\n",
        "        closed_itemsets = []\n",
        "        for k_itemsets in frequent_itemsets:\n",
        "            for itemset in k_itemsets:\n",
        "                if not any([itemset.issubset(superset) and support_data[itemset] == support_data[superset] for superset in closed_itemsets]):\n",
        "                    closed_itemsets.append(itemset)\n",
        "        return closed_itemsets\n",
        "\n",
        "    # Generate custom closed itemsets and write patterns_close.txt\n",
        "    closed_itemsets = generate_closed_itemsets(frequent_itemsets, support_data)\n",
        "    with open('patterns_close.txt', 'w') as f:\n",
        "        for itemset in closed_itemsets:\n",
        "            f.write(f\"{';'.join(list(itemset))} : {support_data[itemset]}\\n\")\n",
        "    sort_and_insert('patterns_close.txt')\n",
        "    insert_total_count_in_file('patterns_close.txt')\n",
        "\n",
        "# Run the custom analysis on a specific file\n",
        "run_custom_analysis('categories.txt')\n"
      ],
      "metadata": {
        "id": "83uF6YilquAH"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}